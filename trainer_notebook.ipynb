{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26525,"status":"ok","timestamp":1662005960716,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"V_pkVGpxsGW9","outputId":"20b97de9-0480-4c43-a6d3-88fbb140810f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21788,"status":"ok","timestamp":1662005982492,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"sDHIYFqB4yTp","outputId":"cee25ec6-7e5f-4e85-e7c0-94d0f975f752"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 68.2 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 53.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 7.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 62.9 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 66.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 75.7 MB/s \n","\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n","\u001b[K     |████████████████████████████████| 116 kB 8.4 MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.6)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n","Collecting portalocker\n","  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.5 portalocker-2.5.1 sacrebleu-2.2.0\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install datasets\n","!pip install sacrebleu"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1662005982493,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"qUQ8MyUK7M-o","outputId":"0fb87607-202f-40af-d2cf-2664e956f69a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel\n"]}],"source":["%cd /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":100287,"status":"ok","timestamp":1657591187989,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"p738n9UmBJ_O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"66b953c3-b6f5-44f9-f375-e33da9897291"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: 100% 897/897 [00:00<00:00, 789kB/s]\n","Downloading: 100% 1.47G/1.47G [00:54<00:00, 28.9MB/s]\n","Downloading: 100% 4.83M/4.83M [00:00<00:00, 7.46MB/s]\n","Downloading: 100% 352k/352k [00:00<00:00, 1.05MB/s]\n"]}],"source":["!python main.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17815,"status":"ok","timestamp":1661219531125,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"GheiniF07S93","outputId":"8b7fce83-245a-4ebd-db1c-9c024d995f98"},"outputs":[{"output_type":"stream","name":"stdout","text":["100% 9275/9275 [00:00<00:00, 39649.32it/s]\n","100% 1988/1988 [00:00<00:00, 39528.20it/s]\n","100% 1987/1987 [00:00<00:00, 12673.17it/s]\n","3\n","['Đ', '/', 'c', 'Nguyên', 'Thi', 'Phong', 'Vu', 'kung', 'pơđam', \"'\", 'bơl', ',', 'tơdrong', 'ngŏ', 'hnhei', ',', 'tơme', 'rong', 'ăn', 'rim', 'ŭnh', 'hnam', 'nă', 'ma', 'ơĭ', 'kông', 'adrĭng', 'teh', 'đak', 'la', 'tơdrong', 'đơ̆ng', 'sơ̆', 'pơgloh', 'alưng', 'liêm', 'đơ̆ng', 'kon', 'kông', 'bơ̆n', ',', 'thoi', \"'\", 'noh', 'la', 'nưih', 'sơngưm', ',', 'trach', 'nhiêm', 'đơ̆ng', 'đĭ', 'đăng', 'kon', 'tơring', ';', 'yua', \"'\", 'noh', ',', 'lư', 'sơnăm', 'anu', 'yuô', 'ơ̆u', ',', 'tinh', 'dach', 'sư̆', 'ăn', 'lư', 'tơdrong', 'ngŏ', 'hnhei', ',', 'tơme', 'rong', 'ăn', 'rim', 'ŭnh', 'hnam', 'nă', 'ma', 'ơĭ', 'kông', 'adrĭng', 'teh', 'đak', 'đơ̆ng', 'lư', 'tơdrong', 'pơm', 'cu', 'thê', 'tơpă', '.']\n","<s>Đ/c Nguyên Thi Phong Vu kung pơđam'bơl, tơdrong ngŏ hnhei, tơme rong ăn rim ŭnh hnam năma ơĭ kông adrĭng teh đak la tơdrong đơ̆ng sơ̆ pơgloh alưng liêm đơ̆ng kon kông bơ̆n, thoi'noh la nưih sơngưm, trach nhiêm đơ̆ng đĭ đăng kon tơring ; yua'noh, lư sơnăm anu yuô ơ̆u, tinh dach sư̆ ăn lư tơdrong ngŏ hnhei, tơme rong ăn rim ŭnh hnam năma ơĭ kông adrĭng teh đak đơ̆ng lư tơdrong pơm cu thê tơpă.</s>\n"]}],"source":["!python custom_dataset/create_custom_tokenizer.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1661219531125,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"-ZS2B95MFvep","outputId":"55b22643-0d47-4f05-cb0a-be13ad847ca1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Aug 23 01:52:10 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324458,"status":"ok","timestamp":1661249029875,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"S8Y_5wxo7XFF","outputId":"a39fdc9b-a3a0-4358-e99f-da060ab64d58"},"outputs":[{"output_type":"stream","name":"stdout","text":["***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","100% 25/25 [01:10<00:00,  2.80s/it]\n","{'eval_loss': 10.697386741638184, 'eval_bleu': 3.5155, 'eval_gen_len': 102.98, 'eval_runtime': 75.2687, 'eval_samples_per_second': 1.329, 'eval_steps_per_second': 0.332}\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 6265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6268\n","{'loss': 15.5708, 'learning_rate': 1.9996809189534145e-05, 'epoch': 0.0}\n","{'loss': 6.0006, 'learning_rate': 1.968091895341417e-05, 'epoch': 0.06}\n","{'loss': 4.3251, 'learning_rate': 1.9361837906828336e-05, 'epoch': 0.13}\n","{'loss': 3.9457, 'learning_rate': 1.9042756860242504e-05, 'epoch': 0.19}\n","{'loss': 3.6895, 'learning_rate': 1.872367581365667e-05, 'epoch': 0.26}\n","{'loss': 3.4889, 'learning_rate': 1.840459476707084e-05, 'epoch': 0.32}\n","  8% 500/6268 [02:23<26:54,  3.57it/s]Saving model checkpoint to /content/checkpoint/viba_bart-finetuned/checkpoint-500\n","Configuration saved in /content/checkpoint/viba_bart-finetuned/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint/viba_bart-finetuned/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/viba_bart-finetuned/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/viba_bart-finetuned/checkpoint-500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/viba_bart-finetuned/checkpoint-500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/viba_bart-finetuned/checkpoint-5500] due to args.save_total_limit\n","{'loss': 3.348, 'learning_rate': 1.8085513720485006e-05, 'epoch': 0.38}\n"," 10% 633/6268 [03:20<28:59,  3.24it/s]Traceback (most recent call last):\n","  File \"trainer/custom_trainer.py\", line 97, in <module>\n","    main()\n","  File \"trainer/custom_trainer.py\", line 90, in main\n","    trainer.train()\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1502, in train\n","    ignore_keys_for_eval=ignore_keys_for_eval,\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1740, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2488, in training_step\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n"," 10% 633/6268 [03:21<29:49,  3.15it/s]\n"]}],"source":["!PYTHONPATH=./ python trainer/custom_trainer.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7J9NvSk-7wPG"},"outputs":[],"source":["from transformers import AutoTokenizer\n","from model.custom_mbart_model import CustomMbartModel\n","from datasets import load_metric\n","from custom_dataset.vi_ba_dataset import ViBaDataset\n","\n","WORD_DROPOUT_RATIO = 0.15\n","WORD_REPLACEMENT_RATIO = 0.15\n","model_checkpoint = \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best_2\"\n","metric = load_metric(\"sacrebleu\")\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = CustomMbartModel.from_pretrained(model_checkpoint)\n","# compute_metric_func = get_metric(metric, tokenizer)\n","\n","_, _, test_dataset = ViBaDataset.get_datasets(data_folder=\"data/all\", tokenizer_path=model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"executionInfo":{"elapsed":17449,"status":"error","timestamp":1661243431124,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"_YH4Jaz88AF2","outputId":"4b66c447-90e0-4cec-aedd-9950c2cf95c4"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 1/1977 [00:06<3:31:11,  6.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Tơdrong pơtho khan tin nă ma kon kông, pơm tơdrong pơm cham pơlĕi anao, tơdrong thu'yŏk, tơdrong thu'yŏk\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 2/1977 [00:17<4:45:57,  8.69s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-4c139e13cd3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m             )\n\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/model/custom_mbart_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dropout_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_replacement_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         )\n\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m                 )\n\u001b[1;32m   1108\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attn_layer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             )\n\u001b[1;32m    453\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch \n","from tqdm import tqdm \n","\n","data = []\n","for i, item in enumerate(tqdm(test_dataset)):\n","    # print(item)\n","    input_ids = item[\"input_ids\"]\n","    labels = item[\"labels\"]\n","    input_ids = torch.tensor([input_ids])\n","    outputs = model.generate(input_ids=input_ids, num_beams=5, max_length=512, num_return_sequences=1)\n","    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    if i % 10 == 0:\n","        print(output)\n","    data.append(output)\n","    with open(\"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/result_2.ba\", \"w\") as f:\n","        f.write(\"\\n\".join(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUQouO8R10v6"},"outputs":[],"source":["!cp -r /content/checkpoint/viba_bart-finetuned/checkpoint-6000 /content/checkpoint-6000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87220,"status":"ok","timestamp":1641213093303,"user":{"displayName":"Nguyên Phạm Quốc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQTsiU5vyfoqXFbBoJ7RAdQp7ITbmtBUJg2weDWg=s64","userId":"12363599076110702021"},"user_tz":-420},"id":"u9iMzYCUT-WU","outputId":"8c4c9f55-77a9-4be6-a79f-dac7b2797d52"},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: content/checkpoint-6000/ (stored 0%)\n","  adding: content/checkpoint-6000/special_tokens_map.json (deflated 50%)\n","  adding: content/checkpoint-6000/config.json (deflated 57%)\n","  adding: content/checkpoint-6000/sentencepiece.bpe.model (deflated 49%)\n","  adding: content/checkpoint-6000/rng_state.pth (deflated 27%)\n","  adding: content/checkpoint-6000/pytorch_model.bin (deflated 7%)\n","  adding: content/checkpoint-6000/tokenizer_config.json (deflated 50%)\n","  adding: content/checkpoint-6000/trainer_state.json (deflated 80%)\n","  adding: content/checkpoint-6000/scheduler.pt (deflated 49%)\n","  adding: content/checkpoint-6000/training_args.bin (deflated 49%)\n","  adding: content/checkpoint-6000/.ipynb_checkpoints/ (stored 0%)\n","  adding: content/checkpoint-6000/dict.txt (deflated 55%)\n"]}],"source":["!zip -r /content/checkpoint6000.zip /content/checkpoint-6000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBgB-Qoy1wYt"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer\n","from model.custom_mbart_model import CustomMbartModel\n","\n","model_checkpoint = \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best_4\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = CustomMbartModel.from_pretrained(model_checkpoint)\n","model.eval()\n","def translate(text):\n","    input_ids = tokenizer(text).input_ids\n","    input_ids = torch.tensor([input_ids])\n","    outputs = model.generate(input_ids=input_ids, num_beams=10, max_length=1024, num_return_sequences=1)\n","    output = tokenizer.decode(outputs[0], skip_special_tokens=True)[0]\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llGprAa-7Md2"},"outputs":[],"source":["import string \n","\n","def norm_text(line):\n","    for n in string.digits:\n","        line = line.replace(n, f\" {n} \")\n","    for p in string.punctuation + \",.\\\\/:;–…\":\n","        line = line.replace(p, f\" {p} \")\n","    while \"  \" in line:\n","        line = line.replace(\"  \", \" \")\n","    line = line.strip()\n","    return line\n","\n","def translate(text):\n","    text = norm_text(text)\n","    input_ids = tokenizer(text).input_ids\n","    input_ids = torch.tensor([input_ids])\n","    outputs = model.generate(input_ids=input_ids, num_beams=5, max_length=256, num_return_sequences=1)\n","    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return output\n"]},{"cell_type":"code","source":["print(translate(\"Tôi là sinh viên đại học Bách Khoa.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3-VRRIQ-sca","executionInfo":{"status":"ok","timestamp":1661248411439,"user_tz":-420,"elapsed":3444,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"9b59e9bb-2545-4b3f-89a7-eb71b2d0c0f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inh la sinh viên Đai hok Bách Sơn.\n"]}]},{"cell_type":"code","source":["print(translate(\"Tôi là sinh viên trường đại học Bách Khoa.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gKz6eD4NHVkC","executionInfo":{"status":"ok","timestamp":1661248414206,"user_tz":-420,"elapsed":2784,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"694d4370-724a-4023-de9b-0f2fb16b4857"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inh la sinh viên trương Đai hok Bách Sơn.\n"]}]},{"cell_type":"code","source":["print(translate(\"Bố tôi là công nhân.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dd7a-94fHX2X","executionInfo":{"status":"ok","timestamp":1661248420280,"user_tz":-420,"elapsed":2896,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"ebcba7ba-ddde-43b3-b7ea-15650bb7264f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["' Bă inh la kơ kuan.\n"]}]},{"cell_type":"code","source":["print(translate(\"Bão 19 đang đổ bộ vào biển Đông năm 2022\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-6A6l1iH1Iy","executionInfo":{"status":"ok","timestamp":1661249165331,"user_tz":-420,"elapsed":5270,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"87f8812e-3130-426a-b377-c3f5f61e0f97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["' Năr minh jĭt tơxĭnh khĕi tơmưt đak tơsĭ Đông sơnăm'băl ơbơ̆u khôn g hriêng'băl\n"]}]},{"cell_type":"code","source":["print(translate(\"Bão 19 đang đổ bộ vào biển Đông.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4FmfORG5H_Op","executionInfo":{"status":"ok","timestamp":1661250002255,"user_tz":-420,"elapsed":3129,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"b893af7d-a9f8-4d24-a993-d7819079f8d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["' Năr minh jĭt tơxĭnh măng đak tơmưt đak tơsĭ Đông.\n"]}]},{"cell_type":"code","source":["print(translate(\"đồng bằng sông Cửu Long ở phía nam Việt Nam.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TidTfjcmIlr5","executionInfo":{"status":"ok","timestamp":1661248668252,"user_tz":-420,"elapsed":4929,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"a7f2976a-0b52-4802-ca93-2395a4507c60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hnam'noh'bă đe hơ - yoh tơ teh nam Viêt Nam.\n"]}]},{"cell_type":"code","source":["print(translate(\"Thành phố Đa Nẵng là một thành phố lớn ở miền Trung\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPByoBKDN4If","executionInfo":{"status":"ok","timestamp":1661250042321,"user_tz":-420,"elapsed":4584,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"5a4304da-7b7a-4e97-f199-f1289e940445"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Đai phô Vinh Thanh la minh thanh phô̆ grông oei apung teh đak\n"]}]},{"cell_type":"code","source":["print(translate(\"thành phố lớn\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWgi2Fd5OLGK","executionInfo":{"status":"ok","timestamp":1661250166218,"user_tz":-420,"elapsed":1724,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"6fec6544-2962-4c97-f434-949d8ce13c45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["thanh phô̆ grông\n"]}]},{"cell_type":"code","source":["print(translate(\"lớn\"))"],"metadata":{"id":"qol7N6xLOfQQ","executionInfo":{"status":"ok","timestamp":1661250178232,"user_tz":-420,"elapsed":1782,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"f1665dcc-9068-4a59-b0e6-50bd0792292d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["grông\n"]}]},{"cell_type":"code","source":["from transformers import MBartForConditionalGeneration\n","\n","bartpho = MBartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best\")\n","TXT = \"Inh la sinh viên đai hŏk <mask> Ho Chi Minh.\"\n","input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n","logits = bartpho(input_ids).logits\n","masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n","probs = logits[0, masked_index].softmax(dim=0)\n","values, predictions = probs.topk(5)\n","print(tokenizer.decode(predictions).split())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOWsU3yTm-zH","executionInfo":{"status":"ok","timestamp":1657616420239,"user_tz":-420,"elapsed":10033,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"}},"outputId":"28416d79-4d2e-4eca-c310-511e42f1ed7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best/config.json\n","Model config MBartConfig {\n","  \"_name_or_path\": \"pretrained/bartpho_syllable\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"architectures\": [\n","    \"CustomMbartModel\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"tokenizer_class\": \"BartphoTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 45783\n","}\n","\n","loading weights file /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best/pytorch_model.bin\n","All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n","\n","All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["['Ho', 'ho', 'tơ', 'Thanh']\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaJb8ugn7gPh"},"outputs":[],"source":["!cp /content/checkpoint6000.zip  /content/drive/MyDrive/BaViBARTModel/checkpoint/viba_bart-finetuned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzFdEvgj8hp-"},"outputs":[],"source":["!cp /content/drive/MyDrive/BaViBARTModel/checkpoint/viba_bart-finetuned/checkpoint6000.zip /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bbgr4H2QdlLG"},"outputs":[],"source":["!rm -r ckpt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15748,"status":"ok","timestamp":1641222161179,"user":{"displayName":"Nguyên Phạm Quốc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQTsiU5vyfoqXFbBoJ7RAdQp7ITbmtBUJg2weDWg=s64","userId":"12363599076110702021"},"user_tz":-420},"id":"XcAZWdl5dRmS","outputId":"79de19fc-c5cd-4f92-c471-61434a0bb038"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  /content/checkpoint6000.zip\n","   creating: /content/ckpt/content/checkpoint-6000/\n","  inflating: /content/ckpt/content/checkpoint-6000/special_tokens_map.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/config.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/sentencepiece.bpe.model  \n","  inflating: /content/ckpt/content/checkpoint-6000/rng_state.pth  \n","  inflating: /content/ckpt/content/checkpoint-6000/pytorch_model.bin  \n","  inflating: /content/ckpt/content/checkpoint-6000/tokenizer_config.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/trainer_state.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/scheduler.pt  \n","  inflating: /content/ckpt/content/checkpoint-6000/training_args.bin  \n","   creating: /content/ckpt/content/checkpoint-6000/.ipynb_checkpoints/\n","  inflating: /content/ckpt/content/checkpoint-6000/dict.txt  \n"]}],"source":["!unzip /content/checkpoint6000.zip -d /content/ckpt "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2199,"status":"ok","timestamp":1661238741888,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"Pw8ud4nCdaKZ","outputId":"e264ff89-d908-409c-e4a3-cf54615d480c"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7407890084728629\n"]}],"source":["import string\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","data_path = \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/result_4.ba\"\n","ref_path = \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/test_sub.ba\"\n","\n","def norm_text(line):\n","    for n in string.digits:\n","        line = line.replace(n, f\" {n} \")\n","    for p in string.punctuation + \",.\\\\/:;–…\":\n","        line = line.replace(p, f\" {p} \")\n","    while \"  \" in line:\n","        line = line.replace(\"  \", \" \")\n","    line = line.strip()\n","    return line\n","\n","def read(path):\n","    data = open(path, \"r\").readlines()\n","    data = [norm_text(item.replace(\"\\n\", \"\")) for item in data]\n","    return data\n","\n","pred = read(data_path)\n","ref = [[item] for item in read(ref_path)]\n","\n","score = corpus_bleu(ref, pred)\n","print(score)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1641394134296,"user":{"displayName":"Nguyên Phạm Quốc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQTsiU5vyfoqXFbBoJ7RAdQp7ITbmtBUJg2weDWg=s64","userId":"12363599076110702021"},"user_tz":-420},"id":"U5-5N6Tttn3o","outputId":"05c43194-c332-4fd5-eeaf-0a45cbbcd3ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["1176 1176\n"]}],"source":["print(len(pred), len(ref))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}