{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16645,"status":"ok","timestamp":1662430373173,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"V_pkVGpxsGW9","outputId":"69a8618c-e844-44e5-b027-c41df8926bb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22340,"status":"ok","timestamp":1662430395510,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"sDHIYFqB4yTp","outputId":"a84d0053-019a-488c-b129-a0b433ddd3a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 6.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 65.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 53.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 67.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.9.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.7.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 55.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 60.0 MB/s \n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n","\u001b[K     |████████████████████████████████| 116 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.6)\n","Collecting portalocker\n","  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.5 portalocker-2.5.1 sacrebleu-2.2.0\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install datasets\n","!pip install sacrebleu"]},{"cell_type":"code","execution_count":106,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1662436983652,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"CaVCbxXg9uUJ"},"outputs":[],"source":["!cp \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/all/valid.vi\" \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/new_all\""]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":478,"status":"ok","timestamp":1662437012642,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"qUQ8MyUK7M-o","outputId":"08e4c432-4877-456f-fd23-4f138ec25494"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel\n"]}],"source":["%cd /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100287,"status":"ok","timestamp":1657591187989,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"p738n9UmBJ_O","outputId":"66b953c3-b6f5-44f9-f375-e33da9897291"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading: 100% 897/897 [00:00<00:00, 789kB/s]\n","Downloading: 100% 1.47G/1.47G [00:54<00:00, 28.9MB/s]\n","Downloading: 100% 4.83M/4.83M [00:00<00:00, 7.46MB/s]\n","Downloading: 100% 352k/352k [00:00<00:00, 1.05MB/s]\n"]}],"source":["!python main.py"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23889,"status":"ok","timestamp":1662445822118,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"GheiniF07S93","outputId":"62279a4a-7646-4e8c-e3ba-ca11981da9e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["100% 9275/9275 [00:00<00:00, 45209.43it/s]\n","100% 1988/1988 [00:00<00:00, 41251.64it/s]\n","100% 1987/1987 [00:00<00:00, 45857.42it/s]\n","100% 3904/3904 [00:00<00:00, 86856.19it/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","ORIGINAL VOCAB 64001\n","NEW VOCAB 64001\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","['Đ@@', '/@@', 'c', 'Nguyên', 'Thi', 'Phong', 'Vu', 'k@@', 'ung', 'p@@', 'ơ@@', 'đam', \"'@@\", 'b@@', 'ơ@@', 'l@@', ',', 'tơ@@', 'd@@', 'rong', 'ng@@', 'ŏ', 'h@@', 'nh@@', 'ei@@', ',', 'tơ@@', 'me', 'rong', 'ăn', 'rim', 'ŭ@@', 'nh', 'h@@', 'nam', 'n@@', 'ă', 'ma', 'ơ@@', 'ĭ', 'kông', 'ad@@', 'r@@', 'ĭ@@', 'ng', 'te@@', 'h', 'đa@@', 'k', 'la', 'tơ@@', 'd@@', 'rong', 'đơ@@', '̆@@', 'ng', 'sơ@@', '̆', 'p@@', 'ơ@@', 'glo@@', 'h', 'al@@', 'ưng', 'liêm', 'đơ@@', '̆@@', 'ng', 'k@@', 'on', 'kông', 'b@@', 'ơ@@', '̆@@', 'n@@', ',', 'thoi', \"'@@\", 'no@@', 'h', 'la', 'n@@', 'ư@@', 'i@@', 'h', 'sơ@@', 'ngư@@', 'm@@', ',', 'tra@@', 'ch', 'nh@@', 'iêm', 'đơ@@', '̆@@', 'ng', 'đ@@', 'ĭ', 'đăng', 'k@@', 'on', 'tơ@@', 'ring@@', ';', 'y@@', 'ua', \"'@@\", 'no@@', 'h@@', ',', 'lư', 'sơ@@', 'năm', 'anu', 'yu@@', 'ô', 'ơ@@', '̆@@', 'u@@', ',', 'tinh', 'da@@', 'ch', 'sư@@', '̆', 'ăn', 'lư', 'tơ@@', 'd@@', 'rong', 'ng@@', 'ŏ', 'h@@', 'nh@@', 'ei@@', ',', 'tơ@@', 'me', 'rong', 'ăn', 'rim', 'ŭ@@', 'nh', 'h@@', 'nam', 'n@@', 'ă', 'ma', 'ơ@@', 'ĭ', 'kông', 'ad@@', 'r@@', 'ĭ@@', 'ng', 'te@@', 'h', 'đa@@', 'k', 'đơ@@', '̆@@', 'ng', 'lư', 'tơ@@', 'd@@', 'rong', 'p@@', 'ơm', 'cu', 'thê', 'tơ@@', 'p@@', 'ă@@', '.']\n","[0, 1782, 7269, 1894, 1759, 5591, 2352, 20319, 1493, 5791, 1487, 9479, 14341, 51899, 1388, 9479, 1494, 4, 25359, 1236, 5604, 1701, 3, 1340, 2008, 14465, 4, 25359, 4407, 5604, 203, 29146, 61058, 5976, 1340, 542, 1301, 30251, 2006, 9479, 3, 42601, 2811, 1698, 3, 2795, 3636, 1664, 20327, 1947, 2644, 25359, 1236, 5604, 26202, 3, 2795, 22178, 3, 1487, 9479, 25999, 1664, 1931, 9389, 34041, 26202, 3, 2795, 1493, 2049, 42601, 1388, 9479, 3, 1301, 4, 43795, 51899, 4323, 1664, 2644, 1301, 11245, 1384, 1664, 22178, 51843, 1387, 4, 5883, 3069, 2008, 21439, 26202, 3, 2795, 3438, 3, 1813, 1493, 2049, 25359, 25245, 65, 1581, 6799, 51899, 4323, 1340, 4, 28859, 22178, 29, 51789, 13346, 2991, 9479, 3, 1881, 4, 5855, 5906, 3069, 25182, 3, 203, 28859, 25359, 1236, 5604, 1701, 3, 1340, 2008, 14465, 4, 25359, 4407, 5604, 203, 29146, 61058, 5976, 1340, 542, 1301, 30251, 2006, 9479, 3, 42601, 2811, 1698, 3, 2795, 3636, 1664, 20327, 1947, 26202, 3, 2795, 28859, 25359, 1236, 5604, 1487, 24254, 13689, 32854, 25359, 1487, 12288, 5, 2]\n","[\"Đ/c Nguyên Thi Phong Vu kung pơđam 'bơl, tơdrong nghnhei, tơme rong ăn rim ŭnh hnam nă ma ơkông adrng teh đak la tơdrong đơng sơpơgloh alưng liêm đơng kon kông bơn, thoi 'noh la nưih sơngưm, trach nhiêm đơng đđăng kon tơring; yua 'noh, lư sơnăm anu yuô ơu, tinh dach sưăn lư tơdrong nghnhei, tơme rong ăn rim ŭnh hnam nă ma ơkông adrng teh đak đơng lư tơdrong pơm cu thê tơpă.\"]\n"]}],"source":["!python custom_dataset/create_custom_bert2bert_tokenizer.py"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":464,"status":"ok","timestamp":1662434268214,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"-ZS2B95MFvep","outputId":"dfe3b4f9-2ef1-4bc3-b37e-e906fd7f2afe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Sep  6 03:17:47 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":118,"metadata":{"executionInfo":{"elapsed":5956,"status":"ok","timestamp":1662445758267,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"0Z88RKQufKjn"},"outputs":[],"source":["!cp -r /content/checkpoint/bert2bert_best_1 \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/\""]},{"cell_type":"code","execution_count":102,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1662436655917,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"ei3nZNHP7DUK"},"outputs":[],"source":["!mv /content/dict.vi \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/all\""]},{"cell_type":"code","execution_count":120,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216620,"status":"ok","timestamp":1662453193710,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"S8Y_5wxo7XFF","outputId":"ac719f12-152b-418a-ab1a-01af179320e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","100% 25/25 [00:50<00:00,  2.00s/it]\n","{'eval_loss': 1.8158351182937622, 'eval_bleu': 13.7894, 'eval_gen_len': 56.82, 'eval_runtime': 54.7901, 'eval_samples_per_second': 1.825, 'eval_steps_per_second': 0.456}\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 9275\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23190\n","{'loss': 2.2881, 'learning_rate': 9.9995687796464e-06, 'epoch': 0.0}\n","  2% 500/23190 [01:51<1:11:26,  5.29it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.76it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.75it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.35it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.10it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.88it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.70it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.66it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.68it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.71it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.79it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.75it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.74it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.69it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.74it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.65it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.50it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.52it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.45it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.40it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.46it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.52it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.8144956827163696, 'eval_bleu': 1.4562, 'eval_gen_len': 16.13, 'eval_runtime': 9.9205, 'eval_samples_per_second': 10.08, 'eval_steps_per_second': 2.52, 'epoch': 0.22}\n","  2% 500/23190 [02:00<1:11:26,  5.29it/s]\n","100% 25/25 [00:09<00:00,  2.51it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-17000] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-23000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.7242, 'learning_rate': 9.56877964639931e-06, 'epoch': 0.43}\n","  4% 1000/23190 [04:05<1:14:15,  4.98it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.70it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.58it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.25it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.96it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.74it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.73it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.69it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.63it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.64it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.38it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.44it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.53it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.54it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.55it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.52it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.57it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.49it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.45it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.52it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.48it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.43it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.47it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.51it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.8044366836547852, 'eval_bleu': 1.1626, 'eval_gen_len': 16.27, 'eval_runtime': 10.2958, 'eval_samples_per_second': 9.713, 'eval_steps_per_second': 2.428, 'epoch': 0.43}\n","  4% 1000/23190 [04:16<1:14:15,  4.98it/s]\n","100% 25/25 [00:09<00:00,  2.50it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-1000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-1000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-1000/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","  6% 1500/23190 [06:20<1:27:00,  4.15it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.64it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.60it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.17it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.90it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.72it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.66it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.61it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.59it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.57it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.51it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.55it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.61it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.67it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.65it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.60it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.64it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.53it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.49it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.45it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.42it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.43it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.43it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.782059907913208, 'eval_bleu': 1.5984, 'eval_gen_len': 16.02, 'eval_runtime': 10.2479, 'eval_samples_per_second': 9.758, 'eval_steps_per_second': 2.44, 'epoch': 0.65}\n","  6% 1500/23190 [06:30<1:27:00,  4.15it/s]\n","100% 25/25 [00:09<00:00,  2.45it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-1500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-1500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-1500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-500] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-1000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.7044, 'learning_rate': 9.13755929279862e-06, 'epoch': 0.86}\n","  9% 2000/23190 [08:33<1:24:26,  4.18it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.67it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.66it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.20it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.96it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.78it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.72it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.65it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.56it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.60it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.57it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.64it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.74it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.78it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.77it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.75it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.79it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.70it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.64it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.67it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.60it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.53it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.58it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.64it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.7740298509597778, 'eval_bleu': 1.7144, 'eval_gen_len': 16.35, 'eval_runtime': 9.8481, 'eval_samples_per_second': 10.154, 'eval_steps_per_second': 2.539, 'epoch': 0.86}\n","  9% 2000/23190 [08:43<1:24:26,  4.18it/s]\n","100% 25/25 [00:09<00:00,  2.58it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-2000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-2000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-2000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-2000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-1500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 11% 2500/23190 [10:47<1:10:22,  4.90it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.73it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.67it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.29it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.07it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.86it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.71it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.65it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.66it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.61it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.72it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.77it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.77it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.75it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.74it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.64it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.59it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.63it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.54it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.46it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.51it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.52it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.7575619220733643, 'eval_bleu': 1.2589, 'eval_gen_len': 16.51, 'eval_runtime': 9.8997, 'eval_samples_per_second': 10.101, 'eval_steps_per_second': 2.525, 'epoch': 1.08}\n"," 11% 2500/23190 [10:57<1:10:22,  4.90it/s]\n","100% 25/25 [00:09<00:00,  2.51it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-2500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-2500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-2500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-2500/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.6255, 'learning_rate': 8.706338939197932e-06, 'epoch': 1.29}\n"," 13% 3000/23190 [13:01<1:09:57,  4.81it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.73it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.78it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.27it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.99it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.79it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.72it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.67it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.63it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.65it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.60it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.62it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.66it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.68it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.65it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.69it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.61it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.53it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.61it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.57it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.48it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.52it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.56it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.7258572578430176, 'eval_bleu': 1.3195, 'eval_gen_len': 16.66, 'eval_runtime': 9.9847, 'eval_samples_per_second': 10.015, 'eval_steps_per_second': 2.504, 'epoch': 1.29}\n"," 13% 3000/23190 [13:11<1:09:57,  4.81it/s]\n","100% 25/25 [00:09<00:00,  2.52it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-3000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-3000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-3000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-3000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-2500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 15% 3500/23190 [15:15<1:15:13,  4.36it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.82it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.77it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.30it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.04it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.81it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.70it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.66it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.61it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.60it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.62it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.68it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.72it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.70it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.77it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.65it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.58it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.59it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.48it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.49it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.58it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.7261550426483154, 'eval_bleu': 1.3842, 'eval_gen_len': 16.22, 'eval_runtime': 9.9339, 'eval_samples_per_second': 10.067, 'eval_steps_per_second': 2.517, 'epoch': 1.51}\n"," 15% 3500/23190 [15:25<1:15:13,  4.36it/s]\n","100% 25/25 [00:09<00:00,  2.54it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-3500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-3500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-3500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-3500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-3000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.5792, 'learning_rate': 8.27511858559724e-06, 'epoch': 1.72}\n"," 17% 4000/23190 [17:29<1:18:33,  4.07it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.77it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.74it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.22it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.02it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.86it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.79it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.73it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.69it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.69it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.65it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.64it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.72it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.73it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.71it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.76it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.67it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.62it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.66it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.59it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.54it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.55it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.63it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.7313045263290405, 'eval_bleu': 1.5188, 'eval_gen_len': 16.09, 'eval_runtime': 9.8024, 'eval_samples_per_second': 10.202, 'eval_steps_per_second': 2.55, 'epoch': 1.72}\n"," 17% 4000/23190 [17:39<1:18:33,  4.07it/s]\n","100% 25/25 [00:09<00:00,  2.56it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-4000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-4000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-4000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-4000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-3500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 19% 4500/23190 [19:42<1:03:30,  4.90it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.50it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.59it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.13it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.92it/s]\u001b[A\n"," 24% 6/25 [00:01<00:07,  2.70it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.59it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.53it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.52it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.56it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.54it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.54it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.60it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.63it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.59it/s]\u001b[A\n"," 64% 16/25 [00:06<00:03,  2.40it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.47it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.44it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.40it/s]\u001b[A\n"," 80% 20/25 [00:07<00:02,  2.45it/s]\u001b[A\n"," 84% 21/25 [00:08<00:01,  2.42it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.36it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.40it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.47it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.7235400676727295, 'eval_bleu': 1.7475, 'eval_gen_len': 16.2, 'eval_runtime': 10.4732, 'eval_samples_per_second': 9.548, 'eval_steps_per_second': 2.387, 'epoch': 1.94}\n"," 19% 4500/23190 [19:53<1:03:30,  4.90it/s]\n","100% 25/25 [00:10<00:00,  2.41it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-4500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-4500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-4500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-4500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-2000] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-4000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.5499, 'learning_rate': 7.84389823199655e-06, 'epoch': 2.16}\n"," 22% 5000/23190 [21:55<57:52,  5.24it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.75it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.84it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.29it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.07it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.89it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.82it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 36% 9/25 [00:02<00:05,  2.72it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.70it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.68it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.76it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.80it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.77it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.76it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.67it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.61it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.67it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.58it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.52it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.59it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.68it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.6561572551727295, 'eval_bleu': 1.497, 'eval_gen_len': 16.32, 'eval_runtime': 9.7132, 'eval_samples_per_second': 10.295, 'eval_steps_per_second': 2.574, 'epoch': 2.16}\n"," 22% 5000/23190 [22:05<57:52,  5.24it/s]\n","100% 25/25 [00:09<00:00,  2.63it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-5000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-5000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-5000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-5000/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 24% 5500/23190 [24:09<1:02:29,  4.72it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.21it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.58it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.20it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.95it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.82it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.70it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.66it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.62it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.66it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.71it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.76it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.65it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.66it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.58it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.53it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.58it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.52it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.45it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.51it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.56it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.6715818643569946, 'eval_bleu': 1.8993, 'eval_gen_len': 16.41, 'eval_runtime': 9.9938, 'eval_samples_per_second': 10.006, 'eval_steps_per_second': 2.502, 'epoch': 2.37}\n"," 24% 5500/23190 [24:19<1:02:29,  4.72it/s]\n","100% 25/25 [00:09<00:00,  2.54it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-5500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-5500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-5500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-5500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-4500] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-5000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.4723, 'learning_rate': 7.412677878395861e-06, 'epoch': 2.59}\n"," 26% 6000/23190 [26:22<1:00:47,  4.71it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.37it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.54it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.13it/s]\u001b[A\n"," 20% 5/25 [00:01<00:07,  2.85it/s]\u001b[A\n"," 24% 6/25 [00:01<00:07,  2.69it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.64it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.60it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.54it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.54it/s]\u001b[A\n"," 44% 11/25 [00:04<00:05,  2.50it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.55it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.61it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.62it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.61it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.58it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.60it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.54it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.46it/s]\u001b[A\n"," 80% 20/25 [00:07<00:02,  2.50it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.45it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.38it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.39it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.44it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.6461101770401, 'eval_bleu': 1.6825, 'eval_gen_len': 16.58, 'eval_runtime': 10.3813, 'eval_samples_per_second': 9.633, 'eval_steps_per_second': 2.408, 'epoch': 2.59}\n"," 26% 6000/23190 [26:32<1:00:47,  4.71it/s]\n","100% 25/25 [00:09<00:00,  2.42it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-6000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-6000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-6000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-6000/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 28% 6500/23190 [28:35<58:02,  4.79it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.49it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.62it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.25it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.95it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.78it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.71it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.63it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.60it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.62it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.59it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.57it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.61it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.66it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.64it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.62it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.65it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.58it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.51it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.47it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.43it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.46it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.50it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.6468863487243652, 'eval_bleu': 1.661, 'eval_gen_len': 16.16, 'eval_runtime': 10.1317, 'eval_samples_per_second': 9.87, 'eval_steps_per_second': 2.468, 'epoch': 2.8}\n"," 28% 6500/23190 [28:45<58:02,  4.79it/s]\n","100% 25/25 [00:09<00:00,  2.49it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-6500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-6500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-6500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-6500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-6500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-6500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-6000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.4572, 'learning_rate': 6.981457524795171e-06, 'epoch': 3.02}\n"," 30% 7000/23190 [30:50<52:52,  5.10it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.79it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.67it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.29it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.03it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.82it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.69it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.63it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.56it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.54it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.60it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.69it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.74it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.75it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.74it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.79it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.70it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.63it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.66it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.60it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.53it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.57it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.63it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.6104434728622437, 'eval_bleu': 1.7823, 'eval_gen_len': 15.98, 'eval_runtime': 9.8254, 'eval_samples_per_second': 10.178, 'eval_steps_per_second': 2.544, 'epoch': 3.02}\n"," 30% 7000/23190 [31:00<52:52,  5.10it/s]\n","100% 25/25 [00:09<00:00,  2.60it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-7000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-7000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-7000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-7000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-7000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-7000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-6500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 32% 7500/23190 [33:01<55:37,  4.70it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.71it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.74it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.38it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.12it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.94it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.88it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.82it/s]\u001b[A\n"," 36% 9/25 [00:02<00:05,  2.79it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.80it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.70it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.73it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.79it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.84it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.83it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.80it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.80it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.72it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.65it/s]\u001b[A\n"," 80% 20/25 [00:06<00:01,  2.70it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.64it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.57it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.61it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.641090750694275, 'eval_bleu': 1.694, 'eval_gen_len': 16.06, 'eval_runtime': 9.6422, 'eval_samples_per_second': 10.371, 'eval_steps_per_second': 2.593, 'epoch': 3.23}\n"," 32% 7500/23190 [33:11<55:37,  4.70it/s]\n","100% 25/25 [00:09<00:00,  2.54it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-7500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-7500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-7500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-7500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-7500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-7500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-7000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3891, 'learning_rate': 6.550237171194481e-06, 'epoch': 3.45}\n"," 34% 8000/23190 [35:15<53:54,  4.70it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.74it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.68it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.32it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.14it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.95it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.87it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 36% 9/25 [00:02<00:05,  2.74it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.76it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.71it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.74it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.81it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.78it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.78it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.78it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.78it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.68it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.59it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.63it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.56it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.45it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.47it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.51it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.627301812171936, 'eval_bleu': 1.7311, 'eval_gen_len': 16.21, 'eval_runtime': 9.8192, 'eval_samples_per_second': 10.184, 'eval_steps_per_second': 2.546, 'epoch': 3.45}\n"," 34% 8000/23190 [35:25<53:54,  4.70it/s]\n","100% 25/25 [00:09<00:00,  2.48it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-8000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-8000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-8000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-8000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-8000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-8000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-7500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 37% 8500/23190 [37:30<1:05:34,  3.73it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  4.67it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.41it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.16it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.91it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.75it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.68it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.61it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.61it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.59it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.56it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.60it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.62it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.65it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.61it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.61it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.65it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.53it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.44it/s]\u001b[A\n"," 80% 20/25 [00:07<00:02,  2.47it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.40it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.40it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.42it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.48it/s]\u001b[A\n","                                          \n","\u001b[A{'eval_loss': 1.6525774002075195, 'eval_bleu': 1.7399, 'eval_gen_len': 16.23, 'eval_runtime': 10.3999, 'eval_samples_per_second': 9.615, 'eval_steps_per_second': 2.404, 'epoch': 3.67}\n"," 37% 8500/23190 [37:40<1:05:34,  3.73it/s]\n","100% 25/25 [00:09<00:00,  2.43it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-8500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-8500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-8500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-8500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-8500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-8500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-8000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.4037, 'learning_rate': 6.119016817593792e-06, 'epoch': 3.88}\n"," 39% 9000/23190 [39:44<55:28,  4.26it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:05,  3.90it/s]\u001b[A\n"," 12% 3/25 [00:01<00:08,  2.71it/s]\u001b[A\n"," 16% 4/25 [00:01<00:08,  2.38it/s]\u001b[A\n"," 20% 5/25 [00:02<00:09,  2.15it/s]\u001b[A\n"," 24% 6/25 [00:02<00:09,  2.01it/s]\u001b[A\n"," 28% 7/25 [00:03<00:09,  1.96it/s]\u001b[A\n"," 32% 8/25 [00:03<00:08,  1.93it/s]\u001b[A\n"," 36% 9/25 [00:04<00:07,  2.03it/s]\u001b[A\n"," 40% 10/25 [00:04<00:06,  2.16it/s]\u001b[A\n"," 44% 11/25 [00:04<00:06,  2.23it/s]\u001b[A\n"," 48% 12/25 [00:05<00:05,  2.31it/s]\u001b[A\n"," 52% 13/25 [00:05<00:04,  2.41it/s]\u001b[A\n"," 56% 14/25 [00:06<00:04,  2.49it/s]\u001b[A\n"," 60% 15/25 [00:06<00:04,  2.50it/s]\u001b[A\n"," 64% 16/25 [00:06<00:03,  2.51it/s]\u001b[A\n"," 68% 17/25 [00:07<00:03,  2.54it/s]\u001b[A\n"," 72% 18/25 [00:07<00:02,  2.47it/s]\u001b[A\n"," 76% 19/25 [00:08<00:02,  2.44it/s]\u001b[A\n"," 80% 20/25 [00:08<00:02,  2.47it/s]\u001b[A\n"," 84% 21/25 [00:08<00:01,  2.39it/s]\u001b[A\n"," 88% 22/25 [00:09<00:01,  2.33it/s]\u001b[A\n"," 92% 23/25 [00:09<00:00,  2.39it/s]\u001b[A\n"," 96% 24/25 [00:10<00:00,  2.41it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.664271593093872, 'eval_bleu': 1.8769, 'eval_gen_len': 16.65, 'eval_runtime': 11.4914, 'eval_samples_per_second': 8.702, 'eval_steps_per_second': 2.176, 'epoch': 3.88}\n"," 39% 9000/23190 [39:56<55:28,  4.26it/s]\n","100% 25/25 [00:10<00:00,  2.40it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-9000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-9000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-9000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-9000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-9000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-9000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-8500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 41% 9500/23190 [41:58<48:46,  4.68it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.65it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.68it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.25it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.99it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.84it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.74it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.68it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.62it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.56it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.35it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.24it/s]\u001b[A\n"," 52% 13/25 [00:04<00:05,  2.23it/s]\u001b[A\n"," 56% 14/25 [00:05<00:05,  2.18it/s]\u001b[A\n"," 60% 15/25 [00:05<00:04,  2.16it/s]\u001b[A\n"," 64% 16/25 [00:06<00:04,  2.12it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.07it/s]\u001b[A\n"," 72% 18/25 [00:07<00:03,  2.02it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.14it/s]\u001b[A\n"," 80% 20/25 [00:08<00:02,  2.31it/s]\u001b[A\n"," 84% 21/25 [00:08<00:01,  2.32it/s]\u001b[A\n"," 88% 22/25 [00:09<00:01,  2.35it/s]\u001b[A\n"," 92% 23/25 [00:09<00:00,  2.36it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.49it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 1.5736852884292603, 'eval_bleu': 1.914, 'eval_gen_len': 16.02, 'eval_runtime': 10.921, 'eval_samples_per_second': 9.157, 'eval_steps_per_second': 2.289, 'epoch': 4.1}\n"," 41% 9500/23190 [42:09<48:46,  4.68it/s]\n","100% 25/25 [00:10<00:00,  2.52it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-9500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-9500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-9500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-9500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-9500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-9500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-5500] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-9000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3508, 'learning_rate': 5.687796463993101e-06, 'epoch': 4.31}\n"," 43% 10000/23190 [44:12<47:36,  4.62it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.64it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.64it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.26it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.98it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.82it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.75it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.72it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.68it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.68it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.66it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.70it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.78it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.77it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.74it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.74it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.62it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.53it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.56it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.46it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.41it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.37it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.21it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.5931777954101562, 'eval_bleu': 2.0168, 'eval_gen_len': 16.08, 'eval_runtime': 10.5083, 'eval_samples_per_second': 9.516, 'eval_steps_per_second': 2.379, 'epoch': 4.31}\n"," 43% 10000/23190 [44:22<47:36,  4.62it/s]\n","100% 25/25 [00:10<00:00,  2.15it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-10000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-10000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-10000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-10000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-10000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-10000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-9500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 45% 10500/23190 [46:26<47:32,  4.45it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.79it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.69it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.26it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.04it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.85it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.79it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.74it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.64it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.65it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.67it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.73it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.76it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.75it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.75it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.64it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.55it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.60it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.56it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.50it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.51it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.58it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.6134334802627563, 'eval_bleu': 2.0818, 'eval_gen_len': 16.38, 'eval_runtime': 9.8767, 'eval_samples_per_second': 10.125, 'eval_steps_per_second': 2.531, 'epoch': 4.53}\n"," 45% 10500/23190 [46:36<47:32,  4.45it/s]\n","100% 25/25 [00:09<00:00,  2.55it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-10500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-10500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-10500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-10500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-10500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-10500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-10000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3268, 'learning_rate': 5.256576110392411e-06, 'epoch': 4.74}\n"," 47% 11000/23190 [48:37<43:28,  4.67it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.77it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.78it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.31it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.04it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.88it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.73it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.70it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.70it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.66it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.72it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.78it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.79it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.81it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.76it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.79it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.69it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.62it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.66it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.58it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.50it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.52it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.59it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.5551303625106812, 'eval_bleu': 2.1305, 'eval_gen_len': 16.22, 'eval_runtime': 9.7563, 'eval_samples_per_second': 10.25, 'eval_steps_per_second': 2.562, 'epoch': 4.74}\n"," 47% 11000/23190 [48:47<43:28,  4.67it/s]\n","100% 25/25 [00:09<00:00,  2.57it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-11000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-11000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-11000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-11000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-11000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-11000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-10500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 50% 11500/23190 [50:50<41:04,  4.74it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.58it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.76it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.28it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.06it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.88it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.80it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.76it/s]\u001b[A\n"," 36% 9/25 [00:02<00:05,  2.74it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.73it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.66it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.69it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.76it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.78it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.74it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.78it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.68it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.60it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.59it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.45it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.41it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.44it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.51it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.559810996055603, 'eval_bleu': 2.2574, 'eval_gen_len': 16.34, 'eval_runtime': 9.9063, 'eval_samples_per_second': 10.095, 'eval_steps_per_second': 2.524, 'epoch': 4.96}\n"," 50% 11500/23190 [51:00<41:04,  4.74it/s]\n","100% 25/25 [00:09<00:00,  2.49it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-11500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-11500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-11500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-11500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-11500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-11500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-11000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3163, 'learning_rate': 4.8253557567917204e-06, 'epoch': 5.17}\n"," 52% 12000/23190 [53:03<35:01,  5.33it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.61it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.65it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.31it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.01it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.84it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.76it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.71it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.69it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.69it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.58it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.61it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.64it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.67it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.65it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.62it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.65it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.55it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.50it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.48it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.42it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.46it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.50it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.5728973150253296, 'eval_bleu': 1.9945, 'eval_gen_len': 16.28, 'eval_runtime': 10.1133, 'eval_samples_per_second': 9.888, 'eval_steps_per_second': 2.472, 'epoch': 5.17}\n"," 52% 12000/23190 [53:13<35:01,  5.33it/s]\n","100% 25/25 [00:09<00:00,  2.47it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-12000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-12000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-12000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-12000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-12000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-12000/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 54% 12500/23190 [55:17<34:16,  5.20it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.42it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.68it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.31it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.98it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.79it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.72it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.68it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.64it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.60it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.69it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.68it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.75it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.72it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.77it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.69it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.62it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.68it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.58it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.54it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.57it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.65it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.5406185388565063, 'eval_bleu': 2.4336, 'eval_gen_len': 16.06, 'eval_runtime': 9.8079, 'eval_samples_per_second': 10.196, 'eval_steps_per_second': 2.549, 'epoch': 5.39}\n"," 54% 12500/23190 [55:27<34:16,  5.20it/s]\n","100% 25/25 [00:09<00:00,  2.63it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-12500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-12500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-12500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-12500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-12500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-12500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-11500] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-12000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.2896, 'learning_rate': 4.394135403191031e-06, 'epoch': 5.61}\n"," 56% 13000/23190 [57:30<36:32,  4.65it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.86it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.77it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.34it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.10it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.90it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.82it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.77it/s]\u001b[A\n"," 36% 9/25 [00:02<00:05,  2.72it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.70it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.65it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.70it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.77it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.80it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.73it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.74it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.77it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.69it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.63it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.66it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.48it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.52it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.61it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.5695381164550781, 'eval_bleu': 1.8488, 'eval_gen_len': 16.03, 'eval_runtime': 9.7344, 'eval_samples_per_second': 10.273, 'eval_steps_per_second': 2.568, 'epoch': 5.61}\n"," 56% 13000/23190 [57:40<36:32,  4.65it/s]\n","100% 25/25 [00:09<00:00,  2.59it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-13000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-13000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-13000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-13000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-13000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-13000/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 58% 13500/23190 [59:44<39:31,  4.09it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.39it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.63it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.23it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.03it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.83it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.77it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.69it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.67it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.68it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.68it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.73it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.80it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.79it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.69it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.65it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.56it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.51it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.60it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.51it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.56it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.64it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 1.5464630126953125, 'eval_bleu': 1.968, 'eval_gen_len': 16.16, 'eval_runtime': 9.8865, 'eval_samples_per_second': 10.115, 'eval_steps_per_second': 2.529, 'epoch': 5.82}\n"," 58% 13500/23190 [59:54<39:31,  4.09it/s]\n","100% 25/25 [00:09<00:00,  2.60it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-13500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-13500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-13500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-13500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-13500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-13500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-13000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.2871, 'learning_rate': 3.962915049590341e-06, 'epoch': 6.04}\n"," 60% 14000/23190 [1:01:57<35:32,  4.31it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  4.99it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.44it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.05it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.88it/s]\u001b[A\n"," 24% 6/25 [00:02<00:07,  2.70it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.63it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.61it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.57it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.56it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.54it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.57it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.68it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.65it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.61it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.64it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.54it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.46it/s]\u001b[A\n"," 80% 20/25 [00:07<00:02,  2.48it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.44it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.39it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.43it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.46it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5577380657196045, 'eval_bleu': 2.531, 'eval_gen_len': 16.26, 'eval_runtime': 10.3077, 'eval_samples_per_second': 9.701, 'eval_steps_per_second': 2.425, 'epoch': 6.04}\n"," 60% 14000/23190 [1:02:08<35:32,  4.31it/s]\n","100% 25/25 [00:09<00:00,  2.42it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-14000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-14000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-14000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-14000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-14000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-14000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-12500] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-13500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 63% 14500/23190 [1:04:11<28:42,  5.05it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.42it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.52it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.13it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.91it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.74it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.70it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.69it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.66it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.67it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.60it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.63it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.66it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.66it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.66it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.66it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.56it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.51it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.58it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.53it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.45it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.52it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.57it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5520199537277222, 'eval_bleu': 2.2622, 'eval_gen_len': 16.25, 'eval_runtime': 10.0793, 'eval_samples_per_second': 9.921, 'eval_steps_per_second': 2.48, 'epoch': 6.25}\n"," 63% 14500/23190 [1:04:22<28:42,  5.05it/s]\n","100% 25/25 [00:09<00:00,  2.55it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-14500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-14500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-14500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-14500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-14500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-14500/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.2588, 'learning_rate': 3.5316946959896508e-06, 'epoch': 6.47}\n"," 65% 15000/23190 [1:06:25<30:15,  4.51it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.31it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.53it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.17it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.93it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.75it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.72it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.69it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.64it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.61it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.58it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.72it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.71it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.73it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.73it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.77it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.68it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.64it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.69it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.65it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.57it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.63it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.67it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5112881660461426, 'eval_bleu': 2.3971, 'eval_gen_len': 16.26, 'eval_runtime': 10.0323, 'eval_samples_per_second': 9.968, 'eval_steps_per_second': 2.492, 'epoch': 6.47}\n"," 65% 15000/23190 [1:06:35<30:15,  4.51it/s]\n","100% 25/25 [00:09<00:00,  2.57it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-15000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-15000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-15000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-15000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-15000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-15000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-14500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 67% 15500/23190 [1:08:39<26:28,  4.84it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.67it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.77it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.34it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.00it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.86it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.73it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.67it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.66it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.62it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.73it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.81it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.69it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.68it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.69it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.61it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.55it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.53it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.48it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.39it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.39it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.45it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5252453088760376, 'eval_bleu': 2.2356, 'eval_gen_len': 15.97, 'eval_runtime': 10.0783, 'eval_samples_per_second': 9.922, 'eval_steps_per_second': 2.481, 'epoch': 6.68}\n"," 67% 15500/23190 [1:08:49<26:28,  4.84it/s]\n","100% 25/25 [00:09<00:00,  2.41it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-15500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-15500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-15500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-15500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-15500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-15500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-15000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.269, 'learning_rate': 3.100474342388961e-06, 'epoch': 6.9}\n"," 69% 16000/23190 [1:10:54<25:36,  4.68it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.62it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.62it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.22it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.02it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.85it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.80it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.71it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.63it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.66it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.56it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.61it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.70it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.76it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.77it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.74it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.76it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.68it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.59it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.64it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.57it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.51it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.54it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.55it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.530464768409729, 'eval_bleu': 2.1911, 'eval_gen_len': 16.11, 'eval_runtime': 9.8719, 'eval_samples_per_second': 10.13, 'eval_steps_per_second': 2.532, 'epoch': 6.9}\n"," 69% 16000/23190 [1:11:03<25:36,  4.68it/s]\n","100% 25/25 [00:09<00:00,  2.54it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-16000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-16000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-16000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-16000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-16000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-16000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-15500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 71% 16500/23190 [1:13:07<23:57,  4.65it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.46it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.50it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.11it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.87it/s]\u001b[A\n"," 24% 6/25 [00:02<00:07,  2.66it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.61it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.56it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.52it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.53it/s]\u001b[A\n"," 44% 11/25 [00:04<00:05,  2.48it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.55it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.64it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.66it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.64it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.63it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.68it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.60it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.54it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.60it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.53it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.46it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.48it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.54it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5050069093704224, 'eval_bleu': 2.1614, 'eval_gen_len': 16.17, 'eval_runtime': 10.1867, 'eval_samples_per_second': 9.817, 'eval_steps_per_second': 2.454, 'epoch': 7.12}\n"," 71% 16500/23190 [1:13:18<23:57,  4.65it/s]\n","100% 25/25 [00:09<00:00,  2.54it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-16500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-16500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-16500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-16500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-16500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-16500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-16000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.24, 'learning_rate': 2.6692539887882713e-06, 'epoch': 7.33}\n"," 73% 17000/23190 [1:15:22<23:34,  4.38it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.73it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.59it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.22it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.93it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.79it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.73it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.61it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.57it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.57it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.54it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.61it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.69it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.77it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.76it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.67it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.72it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.63it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.55it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.59it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.47it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.41it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.41it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.49it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5268762111663818, 'eval_bleu': 2.3322, 'eval_gen_len': 16.37, 'eval_runtime': 10.1252, 'eval_samples_per_second': 9.876, 'eval_steps_per_second': 2.469, 'epoch': 7.33}\n"," 73% 17000/23190 [1:15:32<23:34,  4.38it/s]\n","100% 25/25 [00:09<00:00,  2.48it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-17000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-17000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-17000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-17000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-17000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-17000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-16500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 75% 17500/23190 [1:17:35<20:21,  4.66it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.27it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.49it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.11it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.90it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.74it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.66it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.60it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.51it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.52it/s]\u001b[A\n"," 44% 11/25 [00:04<00:05,  2.48it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.57it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.64it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.69it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.70it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.69it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.75it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.65it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.57it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.62it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.55it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.48it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.53it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.56it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5215389728546143, 'eval_bleu': 2.249, 'eval_gen_len': 16.3, 'eval_runtime': 10.0988, 'eval_samples_per_second': 9.902, 'eval_steps_per_second': 2.476, 'epoch': 7.55}\n"," 75% 17500/23190 [1:17:45<20:21,  4.66it/s]\n","100% 25/25 [00:09<00:00,  2.56it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-17500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-17500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-17500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-17500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-17500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-17500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-17000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3592, 'learning_rate': 2.238033635187581e-06, 'epoch': 7.76}\n"," 78% 18000/23190 [1:19:49<19:17,  4.49it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.37it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.60it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.17it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.89it/s]\u001b[A\n"," 24% 6/25 [00:01<00:07,  2.66it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.60it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.56it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.54it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.51it/s]\u001b[A\n"," 44% 11/25 [00:04<00:05,  2.50it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.54it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.59it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.63it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.62it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.59it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.61it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.47it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.46it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.51it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.45it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.39it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.41it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.48it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.511380672454834, 'eval_bleu': 2.1826, 'eval_gen_len': 16.39, 'eval_runtime': 10.356, 'eval_samples_per_second': 9.656, 'eval_steps_per_second': 2.414, 'epoch': 7.76}\n"," 78% 18000/23190 [1:19:59<19:17,  4.49it/s]\n","100% 25/25 [00:09<00:00,  2.41it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-18000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-18000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-18000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-18000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-18000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-18000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-17500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 80% 18500/23190 [1:22:04<18:48,  4.16it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.96it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.88it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.47it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.16it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.94it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.85it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.75it/s]\u001b[A\n"," 36% 9/25 [00:02<00:05,  2.70it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.70it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.64it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.66it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.69it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.74it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.76it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.76it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.78it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.69it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.55it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.60it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.57it/s]\u001b[A\n"," 88% 22/25 [00:07<00:01,  2.50it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.57it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.65it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5126163959503174, 'eval_bleu': 2.2722, 'eval_gen_len': 16.1, 'eval_runtime': 9.7267, 'eval_samples_per_second': 10.281, 'eval_steps_per_second': 2.57, 'epoch': 7.98}\n"," 80% 18500/23190 [1:22:14<18:48,  4.16it/s]\n","100% 25/25 [00:09<00:00,  2.60it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-18500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-18500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-18500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-18500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-18500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-18500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-18000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3274, 'learning_rate': 1.8068132815868911e-06, 'epoch': 8.19}\n"," 82% 19000/23190 [1:24:18<13:33,  5.15it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.66it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.75it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.37it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.09it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.81it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.73it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.50it/s]\u001b[A\n"," 36% 9/25 [00:03<00:07,  2.21it/s]\u001b[A\n"," 40% 10/25 [00:03<00:07,  2.12it/s]\u001b[A\n"," 44% 11/25 [00:04<00:07,  1.94it/s]\u001b[A\n"," 48% 12/25 [00:04<00:06,  1.93it/s]\u001b[A\n"," 52% 13/25 [00:05<00:06,  1.92it/s]\u001b[A\n"," 56% 14/25 [00:06<00:05,  1.93it/s]\u001b[A\n"," 60% 15/25 [00:06<00:05,  1.94it/s]\u001b[A\n"," 64% 16/25 [00:06<00:04,  2.07it/s]\u001b[A\n"," 68% 17/25 [00:07<00:03,  2.20it/s]\u001b[A\n"," 72% 18/25 [00:07<00:03,  2.23it/s]\u001b[A\n"," 76% 19/25 [00:08<00:02,  2.23it/s]\u001b[A\n"," 80% 20/25 [00:08<00:02,  2.33it/s]\u001b[A\n"," 84% 21/25 [00:09<00:01,  2.31it/s]\u001b[A\n"," 88% 22/25 [00:09<00:01,  2.27it/s]\u001b[A\n"," 92% 23/25 [00:09<00:00,  2.31it/s]\u001b[A\n"," 96% 24/25 [00:10<00:00,  2.37it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5232940912246704, 'eval_bleu': 2.4414, 'eval_gen_len': 16.18, 'eval_runtime': 11.4801, 'eval_samples_per_second': 8.711, 'eval_steps_per_second': 2.178, 'epoch': 8.19}\n"," 82% 19000/23190 [1:24:29<13:33,  5.15it/s]\n","100% 25/25 [00:11<00:00,  2.38it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-19000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-19000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-19000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-19000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-19000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-19000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-18500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 84% 19500/23190 [1:26:33<12:27,  4.93it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.71it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.71it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.30it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.06it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.89it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.74it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.69it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.64it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.65it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.62it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.71it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.76it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.74it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.70it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.72it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.61it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.57it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.61it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.56it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.51it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.54it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.63it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.504306674003601, 'eval_bleu': 2.4134, 'eval_gen_len': 16.11, 'eval_runtime': 9.8672, 'eval_samples_per_second': 10.135, 'eval_steps_per_second': 2.534, 'epoch': 8.41}\n"," 84% 19500/23190 [1:26:43<12:27,  4.93it/s]\n","100% 25/25 [00:09<00:00,  2.56it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-19500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-19500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-19500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-19500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-19500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-19500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-19000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3157, 'learning_rate': 1.375592927986201e-06, 'epoch': 8.62}\n"," 86% 20000/23190 [1:28:46<11:05,  4.79it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.48it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.45it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.02it/s]\u001b[A\n"," 20% 5/25 [00:01<00:07,  2.77it/s]\u001b[A\n"," 24% 6/25 [00:02<00:07,  2.62it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.59it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.57it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.52it/s]\u001b[A\n"," 40% 10/25 [00:03<00:06,  2.50it/s]\u001b[A\n"," 44% 11/25 [00:04<00:05,  2.45it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.51it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.59it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.61it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.61it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.58it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.59it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.51it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.46it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.53it/s]\u001b[A\n"," 84% 21/25 [00:08<00:01,  2.49it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.45it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.48it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.54it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5071951150894165, 'eval_bleu': 2.4294, 'eval_gen_len': 16.12, 'eval_runtime': 10.3616, 'eval_samples_per_second': 9.651, 'eval_steps_per_second': 2.413, 'epoch': 8.62}\n"," 86% 20000/23190 [1:28:56<11:05,  4.79it/s]\n","100% 25/25 [00:09<00:00,  2.47it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-20000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-20000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-20000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-20000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-20000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-20000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-19500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 88% 20500/23190 [1:31:01<10:15,  4.37it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.30it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.58it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.23it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.93it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.75it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.67it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.65it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.63it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.62it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.61it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.65it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.63it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.66it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.65it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.60it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.66it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.57it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.51it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.59it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.50it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.48it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.51it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.57it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.4951928853988647, 'eval_bleu': 2.6615, 'eval_gen_len': 16.26, 'eval_runtime': 10.0592, 'eval_samples_per_second': 9.941, 'eval_steps_per_second': 2.485, 'epoch': 8.84}\n"," 88% 20500/23190 [1:31:11<10:15,  4.37it/s]\n","100% 25/25 [00:09<00:00,  2.56it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-20500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-20500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-20500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-20500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-20500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-20500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-14000] due to args.save_total_limit\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-20000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3174, 'learning_rate': 9.44372574385511e-07, 'epoch': 9.06}\n"," 91% 21000/23190 [1:33:15<07:02,  5.19it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.66it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.55it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.28it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.05it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.82it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.78it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.72it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.68it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.65it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.60it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.63it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.70it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.73it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.73it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.71it/s]\u001b[A\n"," 68% 17/25 [00:06<00:02,  2.73it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.64it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.57it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.62it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.58it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.49it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.52it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.55it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.495675802230835, 'eval_bleu': 2.5147, 'eval_gen_len': 16.09, 'eval_runtime': 9.9369, 'eval_samples_per_second': 10.063, 'eval_steps_per_second': 2.516, 'epoch': 9.06}\n"," 91% 21000/23190 [1:33:25<07:02,  5.19it/s]\n","100% 25/25 [00:09<00:00,  2.52it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-21000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-21000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-21000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-21000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-21000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-21000/added_tokens.json\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 93% 21500/23190 [1:35:30<06:50,  4.12it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.52it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.45it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.10it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.87it/s]\u001b[A\n"," 24% 6/25 [00:01<00:07,  2.71it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.63it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.56it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.49it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.50it/s]\u001b[A\n"," 44% 11/25 [00:04<00:05,  2.46it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.49it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.54it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.57it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.62it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.61it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.63it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.58it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.54it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.59it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.48it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.43it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.47it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.53it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5031894445419312, 'eval_bleu': 2.5218, 'eval_gen_len': 16.14, 'eval_runtime': 10.3231, 'eval_samples_per_second': 9.687, 'eval_steps_per_second': 2.422, 'epoch': 9.27}\n"," 93% 21500/23190 [1:35:40<06:50,  4.12it/s]\n","100% 25/25 [00:09<00:00,  2.50it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-21500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-21500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-21500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-21500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-21500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-21500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-21000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.3055, 'learning_rate': 5.13152220784821e-07, 'epoch': 9.49}\n"," 95% 22000/23190 [1:37:43<04:35,  4.32it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:03,  5.79it/s]\u001b[A\n"," 12% 3/25 [00:00<00:05,  3.69it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.26it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.06it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.87it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.80it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.70it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.62it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.62it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.57it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.60it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.60it/s]\u001b[A\n"," 56% 14/25 [00:04<00:04,  2.62it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.62it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.49it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.52it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.48it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.44it/s]\u001b[A\n"," 80% 20/25 [00:07<00:02,  2.49it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.42it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.37it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.40it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.44it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5033191442489624, 'eval_bleu': 2.3149, 'eval_gen_len': 16.18, 'eval_runtime': 10.2911, 'eval_samples_per_second': 9.717, 'eval_steps_per_second': 2.429, 'epoch': 9.49}\n"," 95% 22000/23190 [1:37:54<04:35,  4.32it/s]\n","100% 25/25 [00:09<00:00,  2.43it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-22000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-22000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-22000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-22000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-22000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-22000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-21500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"," 97% 22500/23190 [1:39:57<02:35,  4.44it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.48it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.60it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.17it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  2.90it/s]\u001b[A\n"," 24% 6/25 [00:01<00:07,  2.67it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.62it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.55it/s]\u001b[A\n"," 36% 9/25 [00:03<00:06,  2.48it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.51it/s]\u001b[A\n"," 44% 11/25 [00:04<00:05,  2.47it/s]\u001b[A\n"," 48% 12/25 [00:04<00:05,  2.51it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.58it/s]\u001b[A\n"," 56% 14/25 [00:05<00:04,  2.61it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.60it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.55it/s]\u001b[A\n"," 68% 17/25 [00:06<00:03,  2.56it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.50it/s]\u001b[A\n"," 76% 19/25 [00:07<00:02,  2.46it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.53it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.47it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.43it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.46it/s]\u001b[A\n"," 96% 24/25 [00:09<00:00,  2.53it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.5088229179382324, 'eval_bleu': 2.4706, 'eval_gen_len': 16.18, 'eval_runtime': 10.3556, 'eval_samples_per_second': 9.657, 'eval_steps_per_second': 2.414, 'epoch': 9.7}\n"," 97% 22500/23190 [1:40:07<02:35,  4.44it/s]\n","100% 25/25 [00:09<00:00,  2.51it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-22500\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-22500/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-22500/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-22500/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-22500/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-22500/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-22000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","{'loss': 1.296, 'learning_rate': 8.19318671841311e-08, 'epoch': 9.92}\n"," 99% 23000/23190 [1:42:11<00:39,  4.87it/s]***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 4\n","\n","  0% 0/25 [00:00<?, ?it/s]\u001b[A\n","  8% 2/25 [00:00<00:04,  5.75it/s]\u001b[A\n"," 12% 3/25 [00:00<00:06,  3.62it/s]\u001b[A\n"," 16% 4/25 [00:01<00:06,  3.24it/s]\u001b[A\n"," 20% 5/25 [00:01<00:06,  3.01it/s]\u001b[A\n"," 24% 6/25 [00:01<00:06,  2.84it/s]\u001b[A\n"," 28% 7/25 [00:02<00:06,  2.77it/s]\u001b[A\n"," 32% 8/25 [00:02<00:06,  2.71it/s]\u001b[A\n"," 36% 9/25 [00:03<00:05,  2.68it/s]\u001b[A\n"," 40% 10/25 [00:03<00:05,  2.69it/s]\u001b[A\n"," 44% 11/25 [00:03<00:05,  2.63it/s]\u001b[A\n"," 48% 12/25 [00:04<00:04,  2.71it/s]\u001b[A\n"," 52% 13/25 [00:04<00:04,  2.78it/s]\u001b[A\n"," 56% 14/25 [00:04<00:03,  2.77it/s]\u001b[A\n"," 60% 15/25 [00:05<00:03,  2.75it/s]\u001b[A\n"," 64% 16/25 [00:05<00:03,  2.71it/s]\u001b[A\n"," 68% 17/25 [00:05<00:02,  2.69it/s]\u001b[A\n"," 72% 18/25 [00:06<00:02,  2.63it/s]\u001b[A\n"," 76% 19/25 [00:06<00:02,  2.56it/s]\u001b[A\n"," 80% 20/25 [00:07<00:01,  2.61it/s]\u001b[A\n"," 84% 21/25 [00:07<00:01,  2.56it/s]\u001b[A\n"," 88% 22/25 [00:08<00:01,  2.50it/s]\u001b[A\n"," 92% 23/25 [00:08<00:00,  2.50it/s]\u001b[A\n"," 96% 24/25 [00:08<00:00,  2.53it/s]\u001b[A\n","                                           \n","\u001b[A{'eval_loss': 1.4874014854431152, 'eval_bleu': 2.4211, 'eval_gen_len': 16.18, 'eval_runtime': 9.9433, 'eval_samples_per_second': 10.057, 'eval_steps_per_second': 2.514, 'epoch': 9.92}\n"," 99% 23000/23190 [1:42:21<00:39,  4.87it/s]\n","100% 25/25 [00:09<00:00,  2.48it/s]\u001b[A\n","                                   \u001b[ASaving model checkpoint to /content/checkpoint/bert2bert_1/checkpoint-23000\n","Configuration saved in /content/checkpoint/bert2bert_1/checkpoint-23000/config.json\n","Model weights saved in /content/checkpoint/bert2bert_1/checkpoint-23000/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_1/checkpoint-23000/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-23000/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_1/checkpoint-23000/added_tokens.json\n","Deleting older checkpoint [/content/checkpoint/bert2bert_1/checkpoint-22500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2340: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n","100% 23190/23190 [1:43:18<00:00,  4.96it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/checkpoint/bert2bert_1/checkpoint-20500 (score: 2.6615).\n","{'train_runtime': 6205.6607, 'train_samples_per_second': 14.946, 'train_steps_per_second': 3.737, 'train_loss': 1.3981275087083502, 'epoch': 10.0}\n","100% 23190/23190 [1:43:25<00:00,  3.74it/s]\n","***** Running Evaluation *****\n","  Num examples = 1977\n","  Batch size = 4\n","100% 495/495 [17:58<00:00,  2.18s/it]\n","{'eval_loss': 1.581681251525879, 'eval_bleu': 19.7434, 'eval_gen_len': 58.4871, 'eval_runtime': 1080.4202, 'eval_samples_per_second': 1.83, 'eval_steps_per_second': 0.458, 'epoch': 10.0}\n","Saving model checkpoint to /content/checkpoint/bert2bert_best_1\n","Configuration saved in /content/checkpoint/bert2bert_best_1/config.json\n","Model weights saved in /content/checkpoint/bert2bert_best_1/pytorch_model.bin\n","tokenizer config file saved in /content/checkpoint/bert2bert_best_1/tokenizer_config.json\n","Special tokens file saved in /content/checkpoint/bert2bert_best_1/special_tokens_map.json\n","added tokens file saved in /content/checkpoint/bert2bert_best_1/added_tokens.json\n"]}],"source":["!PYTHONPATH=./ python trainer/custom_bert2bert_trainer.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ElaoN0T-h78"},"outputs":[],"source":["!cp -r /content/checkpoint/bert2bert_best/ \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79070,"status":"ok","timestamp":1662017842761,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"FJB2I4IF994t","outputId":"bd34a446-8415-4de4-94cc-7da1abeeebe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["cp: error reading '/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/bert2bert-finetuned/checkpoint-19500/optimizer.pt': No such file or directory\n"]}],"source":["!cp -r \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/bert2bert-finetuned\" /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7J9NvSk-7wPG"},"outputs":[],"source":["from transformers import AutoTokenizer\n","from model.custom_mbart_model import CustomMbartModel\n","from datasets import load_metric\n","from custom_dataset.vi_ba_dataset import ViBaDataset\n","\n","WORD_DROPOUT_RATIO = 0.15\n","WORD_REPLACEMENT_RATIO = 0.15\n","model_checkpoint = \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best_2\"\n","metric = load_metric(\"sacrebleu\")\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = CustomMbartModel.from_pretrained(model_checkpoint)\n","# compute_metric_func = get_metric(metric, tokenizer)\n","\n","_, _, test_dataset = ViBaDataset.get_datasets(data_folder=\"data/all\", tokenizer_path=model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"executionInfo":{"elapsed":17449,"status":"error","timestamp":1661243431124,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"_YH4Jaz88AF2","outputId":"4b66c447-90e0-4cec-aedd-9950c2cf95c4"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 1/1977 [00:06<3:31:11,  6.41s/it]"]},{"name":"stdout","output_type":"stream","text":["Tơdrong pơtho khan tin nă ma kon kông, pơm tơdrong pơm cham pơlĕi anao, tơdrong thu'yŏk, tơdrong thu'yŏk\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1977 [00:17<4:45:57,  8.69s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-4c139e13cd3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m             )\n\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/model/custom_mbart_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dropout_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_replacement_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         )\n\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m                 )\n\u001b[1;32m   1108\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attn_layer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             )\n\u001b[1;32m    453\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch \n","from tqdm import tqdm \n","\n","data = []\n","for i, item in enumerate(tqdm(test_dataset)):\n","    # print(item)\n","    input_ids = item[\"input_ids\"]\n","    labels = item[\"labels\"]\n","    input_ids = torch.tensor([input_ids])\n","    outputs = model.generate(input_ids=input_ids, num_beams=5, max_length=512, num_return_sequences=1)\n","    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    if i % 10 == 0:\n","        print(output)\n","    data.append(output)\n","    with open(\"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/result_2.ba\", \"w\") as f:\n","        f.write(\"\\n\".join(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUQouO8R10v6"},"outputs":[],"source":["!cp -r /content/checkpoint/viba_bart-finetuned/checkpoint-6000 /content/checkpoint-6000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87220,"status":"ok","timestamp":1641213093303,"user":{"displayName":"Nguyên Phạm Quốc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQTsiU5vyfoqXFbBoJ7RAdQp7ITbmtBUJg2weDWg=s64","userId":"12363599076110702021"},"user_tz":-420},"id":"u9iMzYCUT-WU","outputId":"8c4c9f55-77a9-4be6-a79f-dac7b2797d52"},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: content/checkpoint-6000/ (stored 0%)\n","  adding: content/checkpoint-6000/special_tokens_map.json (deflated 50%)\n","  adding: content/checkpoint-6000/config.json (deflated 57%)\n","  adding: content/checkpoint-6000/sentencepiece.bpe.model (deflated 49%)\n","  adding: content/checkpoint-6000/rng_state.pth (deflated 27%)\n","  adding: content/checkpoint-6000/pytorch_model.bin (deflated 7%)\n","  adding: content/checkpoint-6000/tokenizer_config.json (deflated 50%)\n","  adding: content/checkpoint-6000/trainer_state.json (deflated 80%)\n","  adding: content/checkpoint-6000/scheduler.pt (deflated 49%)\n","  adding: content/checkpoint-6000/training_args.bin (deflated 49%)\n","  adding: content/checkpoint-6000/.ipynb_checkpoints/ (stored 0%)\n","  adding: content/checkpoint-6000/dict.txt (deflated 55%)\n"]}],"source":["!zip -r /content/checkpoint6000.zip /content/checkpoint-6000"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":739,"status":"ok","timestamp":1662430927199,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"lusLK6WEmTWo","outputId":"0a595252-126d-4232-af92-94bf525bb6bb"},"outputs":[{"data":{"text/plain":["['Thanh',\n"," 'phô',\n"," 'Quy',\n"," 'Nh@@',\n"," 'ơ@@',\n"," 'n@@',\n"," ',',\n"," 'Thi',\n"," 'sa',\n"," 'An',\n"," 'Nh@@',\n"," 'ơ@@',\n"," 'n@@',\n"," ',',\n"," 'Hŭn',\n"," 'Tây',\n"," 'Sơ@@',\n"," 'n@@',\n"," ',',\n"," 'Hŭn',\n"," 'Vinh',\n"," 'Tha@@',\n"," 'nh@@',\n"," ',',\n"," 'Hŭn',\n"," 'Vân',\n"," 'Canh']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"pretrained/bert2bert\")\n","l = \"Thanh phô Quy Nhơn, Thi sa An Nhơn, Hŭn Tây Sơn, Hŭn Vinh Thanh, Hŭn Vân Canh\"\n","# a = tokenizer.encode(l)\n","# b = tokenizer.batch_decode([a], skip_special_tokens=True)\n","tokenizer.tokenize(l)"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1202,"status":"ok","timestamp":1662435369707,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"YnF9jt_1m5Ht","outputId":"a9d28996-c983-4502-899c-a29f53caac3a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["['Tôi', 'là', 'sinh', 'viên', 'đại', 'học']\n","['Tôi là sinh viên đại học']\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModel\n","\n","from transformers import EncoderDecoderModel\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","# print(type(tokenizer))\n","# print(type(model))\n","l = \"Tôi là sinh viên đại học\"\n","a = tokenizer.encode(l)\n","b = tokenizer.batch_decode([a], skip_special_tokens=True)\n","print(tokenizer.tokenize(l))\n","print(b)"]},{"cell_type":"code","execution_count":121,"metadata":{"executionInfo":{"elapsed":13993,"status":"ok","timestamp":1662453222998,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"iBgB-Qoy1wYt"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer\n","from model.custom_bert2bert_model import CustomBERT2BERTModel\n","\n","# tokenizer_input = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","# tokenizer_output = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n","\n","model_checkpoint = \"/content/checkpoint/bert2bert_best_1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = CustomBERT2BERTModel.from_pretrained(model_checkpoint)\n","model.eval()\n","def translate(text):\n","    input_ids = tokenizer(text).input_ids\n","    input_ids = torch.tensor([input_ids])\n","    outputs = model.generate(input_ids=input_ids, num_beams=10, max_length=256, num_return_sequences=1)\n","    output = tokenizer.decode(outputs[0], skip_special_tokens=True)[0]\n","    return output\n"]},{"cell_type":"code","execution_count":125,"metadata":{"executionInfo":{"elapsed":300,"status":"ok","timestamp":1662453238594,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"llGprAa-7Md2"},"outputs":[],"source":["import string \n","\n","def norm_text(line):\n","    for n in string.digits:\n","        line = line.replace(n, f\" {n} \")\n","    for p in string.punctuation + \",.\\\\/:;–…\":\n","        line = line.replace(p, f\" {p} \")\n","    while \"  \" in line:\n","        line = line.replace(\"  \", \" \")\n","    line = line.strip()\n","    return line\n","\n","def translate(text):\n","    text = norm_text(text)\n","    input_ids = tokenizer(text).input_ids\n","    input_ids = torch.tensor([input_ids])\n","    outputs = model.generate(input_ids=input_ids, num_beams=5, max_length=256, num_return_sequences=1)\n","    output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n","    return output\n"]},{"cell_type":"code","execution_count":126,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4506,"status":"ok","timestamp":1662453244163,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"G3-VRRIQ-sca","outputId":"c6028d97-bad1-449e-a6c4-89469225c5e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> <s>'Boŏk la bơngai pơtơtơtơtơtơtơtơtơtơtơtơtơtơtơ<unk> p </s>\n"]}],"source":["print(translate(\"Tôi là sinh viên đại học Bách Khoa.\"))"]},{"cell_type":"code","execution_count":127,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5408,"status":"ok","timestamp":1662453251154,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"gKz6eD4NHVkC","outputId":"554a1271-fbd4-47d7-eebd-ba9dea1cc3f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> <s>'Boŏk la bơngai pơtơtơtơtơtơtơtơtơtơtơtơtơtơtơ<unk> p </s>\n"]}],"source":["print(translate(\"Tôi là sinh viên trường đại học Bách Khoa.\"))"]},{"cell_type":"code","execution_count":128,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2860,"status":"ok","timestamp":1662453254006,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"dd7a-94fHX2X","outputId":"4812012f-e49c-49d1-a952-feb6f79bd138"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> <s>'Boŏng'bă ngai ơ<unk> tơdrong pơm. </s>\n"]}],"source":["print(translate(\"Bố tôi là công nhân.\"))"]},{"cell_type":"code","execution_count":129,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7743,"status":"ok","timestamp":1662453270287,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"S-6A6l1iH1Iy","outputId":"ffcc9b38-6f00-49fa-e36f-106d889d1122"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> <s> Đnăm'băl ơbơ<unk> u không hriêng minh j<unk> t tơx<unk> nh truh sơnăm'băl ơbơ<unk> u không hriêng'băl j<unk> t </s>\n"]}],"source":["print(translate(\"Bão 19 đang đổ bộ vào biển Đông năm 2022\"))"]},{"cell_type":"code","execution_count":130,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6780,"status":"ok","timestamp":1662453290080,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"4FmfORG5H_Op","outputId":"a47deaf2-8464-4ef8-901e-e8810e78e473"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> <s> Đ' băl j<unk> t drơ<unk> u tơmưt tơx<unk> nh j<unk> t tram'băl hriêng tơx<unk> nh j<unk> t tơhngam </s>\n"]}],"source":["print(translate(\"Bão 19 đang đổ bộ vào biển Đông.\"))"]},{"cell_type":"code","execution_count":131,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4803,"status":"ok","timestamp":1662453296234,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"TidTfjcmIlr5","outputId":"a1922948-a992-4afb-f1f4-0926d04157dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> <s> Kơk xuôn u<unk> i kơ<unk> k đak tơs<unk>. </s>\n"]}],"source":["print(translate(\"đồng bằng sông Cửu Long ở phía nam Việt Nam.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3286,"status":"ok","timestamp":1662028478920,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"JPByoBKDN4If","outputId":"939aac4f-2d21-4750-b3dd-a3e77eaec6eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["thanh phô phơ la la minh thanh thanh grông uĕi apung ng ng ng\n"]}],"source":["print(translate(\"Thành phố Đa Nẵng là một thành phố lớn ở miền Trung\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1122,"status":"ok","timestamp":1662028488641,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"GWgi2Fd5OLGK","outputId":"dd776930-ddd5-4755-a2f5-a69cfcd1ef3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["thanh grông\n"]}],"source":["print(translate(\"thành phố lớn\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":983,"status":"ok","timestamp":1662028492047,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"qol7N6xLOfQQ","outputId":"a439a557-c86a-4df8-aa5a-a6576913696d"},"outputs":[{"name":"stdout","output_type":"stream","text":["grông\n"]}],"source":["print(translate(\"lớn\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10033,"status":"ok","timestamp":1657616420239,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"wOWsU3yTm-zH","outputId":"28416d79-4d2e-4eca-c310-511e42f1ed7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best/config.json\n","Model config MBartConfig {\n","  \"_name_or_path\": \"pretrained/bartpho_syllable\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"architectures\": [\n","    \"CustomMbartModel\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"tokenizer_class\": \"BartphoTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 45783\n","}\n","\n","loading weights file /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best/pytorch_model.bin\n","All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n","\n","All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["['Ho', 'ho', 'tơ', 'Thanh']\n"]}],"source":["from transformers import MBartForConditionalGeneration\n","\n","bartpho = MBartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/checkpoint/best\")\n","TXT = \"Inh la sinh viên đai hŏk <mask> Ho Chi Minh.\"\n","input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n","logits = bartpho(input_ids).logits\n","masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n","probs = logits[0, masked_index].softmax(dim=0)\n","values, predictions = probs.topk(5)\n","print(tokenizer.decode(predictions).split())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaJb8ugn7gPh"},"outputs":[],"source":["!cp /content/checkpoint6000.zip  /content/drive/MyDrive/BaViBARTModel/checkpoint/viba_bart-finetuned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzFdEvgj8hp-"},"outputs":[],"source":["!cp /content/drive/MyDrive/BaViBARTModel/checkpoint/viba_bart-finetuned/checkpoint6000.zip /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bbgr4H2QdlLG"},"outputs":[],"source":["!rm -r ckpt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15748,"status":"ok","timestamp":1641222161179,"user":{"displayName":"Nguyên Phạm Quốc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQTsiU5vyfoqXFbBoJ7RAdQp7ITbmtBUJg2weDWg=s64","userId":"12363599076110702021"},"user_tz":-420},"id":"XcAZWdl5dRmS","outputId":"79de19fc-c5cd-4f92-c471-61434a0bb038"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  /content/checkpoint6000.zip\n","   creating: /content/ckpt/content/checkpoint-6000/\n","  inflating: /content/ckpt/content/checkpoint-6000/special_tokens_map.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/config.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/sentencepiece.bpe.model  \n","  inflating: /content/ckpt/content/checkpoint-6000/rng_state.pth  \n","  inflating: /content/ckpt/content/checkpoint-6000/pytorch_model.bin  \n","  inflating: /content/ckpt/content/checkpoint-6000/tokenizer_config.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/trainer_state.json  \n","  inflating: /content/ckpt/content/checkpoint-6000/scheduler.pt  \n","  inflating: /content/ckpt/content/checkpoint-6000/training_args.bin  \n","   creating: /content/ckpt/content/checkpoint-6000/.ipynb_checkpoints/\n","  inflating: /content/ckpt/content/checkpoint-6000/dict.txt  \n"]}],"source":["!unzip /content/checkpoint6000.zip -d /content/ckpt "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2199,"status":"ok","timestamp":1661238741888,"user":{"displayName":"Nguyên Phạm Quốc","userId":"12363599076110702021"},"user_tz":-420},"id":"Pw8ud4nCdaKZ","outputId":"e264ff89-d908-409c-e4a3-cf54615d480c"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7407890084728629\n"]}],"source":["import string\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","data_path = \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/result_4.ba\"\n","ref_path = \"/content/drive/MyDrive/Thac Si/Thesis/BaViBARTModel/data/test_sub.ba\"\n","\n","def norm_text(line):\n","    for n in string.digits:\n","        line = line.replace(n, f\" {n} \")\n","    for p in string.punctuation + \",.\\\\/:;–…\":\n","        line = line.replace(p, f\" {p} \")\n","    while \"  \" in line:\n","        line = line.replace(\"  \", \" \")\n","    line = line.strip()\n","    return line\n","\n","def read(path):\n","    data = open(path, \"r\").readlines()\n","    data = [norm_text(item.replace(\"\\n\", \"\")) for item in data]\n","    return data\n","\n","pred = read(data_path)\n","ref = [[item] for item in read(ref_path)]\n","\n","score = corpus_bleu(ref, pred)\n","print(score)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1641394134296,"user":{"displayName":"Nguyên Phạm Quốc","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQTsiU5vyfoqXFbBoJ7RAdQp7ITbmtBUJg2weDWg=s64","userId":"12363599076110702021"},"user_tz":-420},"id":"U5-5N6Tttn3o","outputId":"05c43194-c332-4fd5-eeaf-0a45cbbcd3ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["1176 1176\n"]}],"source":["print(len(pred), len(ref))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
